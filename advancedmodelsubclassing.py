# -*- coding: utf-8 -*-
"""AdvancedModelSubclassing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NSAx0B5Oay-Et6VK91ZyqXLjgX1hdHaB
"""

from tensorflow.keras.models import Model
import tensorflow as tf
from tensorflow.keras.layers import Dense,Dropout,Layer
from tensorflow.keras.utils import to_categorical

class MyModel(Model):
  def __init__(self, num_classes, **kwargs):
    super(MyModel,self).__init__(**kwargs)
    self.dense1 = Dense(16,activation=tf.nn.sigmoid)
    self.dropout = Dropout(0.5)
    self.dense2 = Dense(num_classes,activation=tf.nn.softmax)

  
  def call(self,inputs,training=False):
    h = self.dense1(inputs)
    y = self.dense1(inputs)
    h = self.dropout(h,training=training)
    fin = tf.concat([h,y],axis=0)
    return self.dense2(fin)


my_model = MyModel(12,name="Lol")
my_model(tf.random.uniform([1,10]))
my_model.summary()

class Sigmoid(Layer):
  """Bisa jadi Salah ,Masi Belajar aku """
  def __init__(self,input_dim,units):
    super().__init__()
    weight_init = tf.random_normal_initializer()
    bias_init = tf.zeros(shape=(units))
    self.w = tf.Variable(initial_value=weight_init(shape=(input_dim,units)))
    self.b = tf.Variable(initial_value=bias_init)
  def call(self, inputs):
    matmul = tf.add(tf.matmul(inputs, self.w) , self.b)
    return 1 / (1 + tf.exp(-matmul)) 

class DenseCustom(Layer):
  def __init__(self,units,**kwargs):
    super().__init__(**kwargs)
    self.units= units

  def build(self,input_shape):
    print(input_shape)
    self.weight = self.add_weight(shape=(input_shape[-1], self.units),
                                  initializer=tf.random.normal)
    self.bias = self.add_weight(shape=(self.units),
                                initializer=tf.zeros)
  def call(self,inputs): 
    return tf.matmul(inputs,self.weight) + self.bias


sig = Sigmoid(15,4)
inputs = tf.ones((3,15))
print(sig(inputs))

Dens = DenseCustom(5)
x = tf.ones(shape=(2,10))
print(Dens(x))

def layerMean(Layer):
  def __init__(self,units,input_dim):
    super().__init__()
    self.weight = self.add_weight(shape=(input_dim,units),initializer=tf.random.normal)
    self.bias = self.add_weight(shape=(units),initializer=tf.ones)
    self.sum_Activation = tf.Variable(initial_value=tf.zeros(shape=(units)),trainable=False)
    self.num_Calls = tf.Variable(initial_value=0,trainable=False)

  def call(self,inputs):
    activations = tf.matmul(inputs,self.weight) + self.bias
    self.sum_Activation.assign_add(tf.reduce_mean(activations,axis=0))

x = tf.constant(2.0)

with tf.GradientTape() as tape:
  tape.watch(x)
  y = x**2
  grad = tape.gradient(target=y,sources=x) 

print(grad)

x = tf.constant([0,1,2,3],dtype=tf.float32)

with tf.GradientTape() as tape:
  tape.watch(x)
  print(tf.reduce_sum(x**2,axis=None))
  y = tf.reduce_sum(x**2)
  z = tf.math.sin(y)
  dz_dy = tape.gradient(target=z,sources=y)

print(dz_dy)

x = tf.constant([0,1,2,3],dtype=tf.float32)

with tf.GradientTape() as tape:
  tape.watch(x)
  print(tf.reduce_sum(x**2,axis=None))
  y = tf.reduce_sum(x**2)
  z = tf.math.sin(y)
  dz_dy,dz_dx = tape.gradient(target=z,sources=[y,x])

print(dz_dy)
print(dz_dx)

import matplotlib.pyplot as plt
import numpy as np

def atLeastNoisy(m,b,n = 100):
  x =tf.random.uniform(shape=(n,))
  noise = tf.random.normal(shape=(len(x),),stddev=0.12)
  y = m * x +b + noise
  return x,y

m= 1
b = 2
x_train,y_train = atLeastNoisy(m,b)
plt.plot(x_train,y_train,"bo")

class LinLayer(Layer):
  def __init__(self,**kwargs):
    super().__init__(**kwargs)

  def build(self,input_shape):
    self.weight = self.add_weight(shape=(1,),
                                  initializer=tf.random.uniform)
    self.bias = self.add_weight(shape=(1,),
                                initializer=tf.ones)

  def call(self,inputs):
    return inputs * self.weight + self.bias

linReg = LinLayer()
linReg(x_train)

def squaredError(y_pred,y_true):
  return tf.reduce_mean(tf.square(y_pred - y_true))

loss = squaredError(linReg(x_train),y_train)
print("starting loss", loss.numpy())

learning_rate =0.1
steps=35

for step in range(steps):
  with tf.GradientTape() as tape:
    predictions = linReg(x_train)
    loss = squaredError(predictions,y_train)
  
  gradients = tape.gradient(loss,linReg.trainable_variables)
  linReg.weight.assign_add(learning_rate*gradients[0])
  linReg.bias.assign_add(learning_rate*gradients[1])

plt.figure(figsize=(10,8))
print("m:{}, Trained m :{}".format(m,linReg.weight.numpy()))
print("b:{}, Trained b :{}".format(m,linReg.bias.numpy()))

plt.plot(x_train,y_train,"b.")

x_linear_reg =np.linspace(min(x_train),max(x_train),20)
plt.plot(x_linear_reg,linReg.weight * x_linear_reg + linReg.bias,'r.')

print("m:{}, Trained m :{}".format(m,linReg.weight.numpy()))
print("b:{}, Trained b :{}".format(m,linReg.bias.numpy()))

plt.plot(x_train,y_train,"b.")

x_linear_reg =np.linspace(min(x_train),max(x_train),100)
plt.plot(x_linear_reg,linReg.weight * x_linear_reg + linReg.bias,'r.')

#Lets Try IT

class TryCustomDense(Layer):

  def __init__(self,units):
    super(TryCustomDense,self).__init__()
    self.units = units

  def build(self, input_shape):
    self.w = self.add_weight(shape=(input_shape[-1],self.units),
                             initializer=tf.random_normal_initializer,
                             name="kernel")
    self.b = self.add_weight(shape=(self.units),initializer='zeros',
                              name="bias")
    
  def call(self,inputs):
    return tf.matmul(inputs, self.w) + self.b


class CustomDropout(Layer):
  def __init__(self, rate):
    super(CustomDropout,self).__init__()
    self.rate = rate

  def call(self,inputs):
    return tf.nn.dropout(inputs,rate=self.rate)
  

class SubclassModel(Model):
  def __init__(self,units_1,units_2,units_3):
    super(SubclassModel,self).__init__()
    self.layer_1 = TryCustomDense(units_1)
    self.dropout = CustomDropout(0.5)
    self.layer_2 = TryCustomDense(units_2)
    self.layer_3 = TryCustomDense(units_3)
    self.softmax = tf.keras.layers.Softmax()

  def call(self,inputs):
    x = self.layer_1(inputs)
    x = tf.nn.relu(x)
    x = self.dropout(x)
    x = self.layer_2(x)
    x = tf.nn.relu(x)
    x = self.dropout(x)
    x = self.layer_3(x)

    return self.softmax(x)

model = SubclassModel(64,64,46)
print(model(tf.ones((1,10000))))
model.summary()

from tensorflow.keras.datasets import reuters

(train_data,train_labels),(test_data,test_labels) = reuters.load_data(num_words=10000)

class_names = ['cocoa','grain','veg-oil','earn','acq','wheat','copper','housing','money-supply',
   'coffee','sugar','trade','reserves','ship','cotton','carcass','crude','nat-gas',
   'cpi','money-fx','interest','gnp','meal-feed','alum','oilseed','gold','tin',
   'strategic-metal','livestock','retail','ipi','iron-steel','rubber','heat','jobs',
   'lei','bop','zinc','orange','pet-chem','dlr','gas','silver','wpi','hog','lead']

print(f"label {class_names[train_labels[0]]}")

print(train_data[0])
print(train_labels[0])

word_to_indx = reuters.get_word_index()
print(word_to_indx)

inverted_word_index = dict([(value,key) for key,value in word_to_indx.items()])
text_news = " ".join(inverted_word_index.get(i ,"?") for i in train_data[0])
text_news

def bag_of_words(text_samples, max_elements=10000):
  output = np.zeros(shape=(len(text_samples),max_elements))
  for i,word in enumerate(text_samples):
    output[i,word] = 1
  return output


x_train = bag_of_words(train_data)
x_test = bag_of_words(test_data)
print(type(x_train))
print("Shape of x_train:" ,x_train.shape)
print("Shape Of x_test:" ,x_test.shape)

print(x_train[0,0:100])

print(train_labels[0])

# to_categorical(train_lables,num_classes=len(class_names))



loss_obj = tf.keras.losses.SparseCategoricalCrossentropy()
optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)
train_dataset = tf.data.Dataset.from_tensor_slices((x_train,train_labels))
train_dataset = train_dataset.batch(32)



def loss(model,x,y,wd):
  kernel_variables = []
  for l in model.layers:
    for w in l.weights:
      if 'kernel' in w.name:
        kernel_variables.append(w)
  wd_penalty = wd *tf.reduce_sum([tf.reduce_sum(tf.square(k)) for k in kernel_variables])
  wd_penalty = tf.cast(wd_penalty,tf.float32)
  yPred = model(x)
  return loss_obj(y_true=y,y_pred=yPred) +wd_penalty

def gradient(model,inputs,targets,wd):
  with tf.GradientTape() as tape:
    loss_val = loss(model,inputs,targets,wd)
      
  return loss_val, tape.gradient(loss_val,model.trainable_variables)

def train():
  train_loss_result = []
  train_accuracy = []

  num_epochs = 10
  weight_decay = 0.001
  for epoch in range(num_epochs):

    epoch_loss_avg = tf.keras.metrics.Mean()
    epoch_accuracy = tf.keras.metrics.CategoricalAccuracy()

    for x,y in train_dataset:
      loss_value, grads = gradient(model, x,y,weight_decay)
      optimizer.apply_gradients(zip(grads,model.trainable_variables))
      
      epoch_loss_avg(loss_value)
      epoch_accuracy(to_categorical(y),model(x))

    train_loss_result.append(epoch_loss_avg.result())
    train_accuracy.append(epoch_accuracy.result())

    print(f"Epoch : {epoch}, Loss : {epoch_loss_avg.result()} acc : {epoch_accuracy.result()} ")

train()

